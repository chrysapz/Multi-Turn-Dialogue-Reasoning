{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f892ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae59fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_mapping = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3\n",
    "}\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self, sample_id, answer, options, article, generated_utterances=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.answer = answer\n",
    "        self.options = options\n",
    "        self.article = article\n",
    "        \n",
    "        self.label_descr = options[answer_mapping[answer]]\n",
    "        \n",
    "        self.generated_utterances = generated_utterances or []\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.generated_utterances:\n",
    "            return f\"Sample ID: {self.sample_id}\\nAnswer Number: {self.answer}\\nAnswer: {self.label_descr}\\nOptions: {self.options}\\nContext: {self.article}\\nGenerated Utterances: {self.generated_utterances}\"\n",
    "        else:\n",
    "            return f\"Sample ID: {self.sample_id}\\nAnswer Number: {self.answer}\\nAnswer: {self.label_descr}\\nOptions: {self.options}\\nContext: {self.article}\"\n",
    "            \n",
    "def load_sample_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.loads(file.read())\n",
    "    return Sample(\n",
    "        data['id'],\n",
    "        data['answers'],\n",
    "        data['options'],\n",
    "        data['article']\n",
    "    )\n",
    "\n",
    "def load_all_samples(base_dir,mode):\n",
    "    data_dir = os.path.join(base_dir, mode)\n",
    "    samples = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            sample = load_sample_from_file(os.path.join(data_dir, filename))\n",
    "            samples.append(sample)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b006128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages(samples):\n",
    "    total_word_count = 0\n",
    "    total_sentences = 0\n",
    "    total_turns = 0\n",
    "\n",
    "    for sample in samples:\n",
    "        # average sentence length for the responses\n",
    "        for option in sample.options:\n",
    "            total_word_count += len(option.split())\n",
    "            total_sentences += 1\n",
    "\n",
    "        # average turns in a conversation\n",
    "        turns = sample.article.split('.')\n",
    "        total_turns += len([t for t in turns if t.strip()])  # Filtering out any empty turns\n",
    "\n",
    "    avg_sentence_length = total_word_count / total_sentences\n",
    "    avg_turns = total_turns / len(samples)\n",
    "\n",
    "    return avg_sentence_length, avg_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1220c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: train_5844\n",
      "Answer Number: A\n",
      "Answer: m : alright . have a nice time shopping with your mother then .\n",
      "Options: ['m : alright . have a nice time shopping with your mother then .', \"m : let 's have breakfast first and then ride along the lake . see you in a bit !\", \"m : i did n't know you went shopping with your mother yesterday .\", 'm : have a nice time celebrating the new year with your mother this morning .']\n",
      "Context: m : would you like to go to right along the lake with me this morning ? f : i 'd like to but i have to go to the city mall with my mother . the new year is coming .\n",
      "   \n",
      "Sample ID: train_4582\n",
      "Answer Number: A\n",
      "Answer: f : it must be something wrong with the engine so you could n't get your car going .\n",
      "Options: [\"f : it must be something wrong with the engine so you could n't get your car going .\", 'f : how much did it cost you to repair your car ?', \"f : i 'm sorry to hear that you had an accident this morning , is everything all right ?\", \"f : since you could n't get your car going , you were late for the movie .\"]\n",
      "Context: f : did you come to work on time ? m : no , i came late because my car would n't start .\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"data/mutual\"\n",
    "train_samples = load_all_samples(base_dir, \"train\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(train_samples[i])\n",
    "    print(\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e40cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length of responses: 17.87 words\n",
      "Average turns per conversation: 6.66\n"
     ]
    }
   ],
   "source": [
    "avg_sentence_length, avg_turns = compute_averages(train_samples)\n",
    "\n",
    "print(f\"Average sentence length of responses: {avg_sentence_length:.2f} words\")\n",
    "print(f\"Average turns per conversation: {avg_turns:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5eccc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ea4e0e9f9d4103baf1c746ab98b1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68598e2e63b4a7197d33902eca0636a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb5e098c8d644918500fab37d0b5558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c0b3c6446f4ab29acf7ce793787b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b896ad89ba604329a2939ed7203133bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch \n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "next_utterance_approx_len = 30\n",
    "\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "def generate_utterances_for_sample(sample, num_sequences=3):\n",
    "   \n",
    "    input_text = sample.article\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device) \n",
    "\n",
    "    output = model.generate(input_ids, max_length=len(input_ids[0]) + next_utterance_approx_len, num_return_sequences=num_sequences, pad_token_id=tokenizer.eos_token_id,\n",
    "                           num_beams=3, do_sample=True,top_k=50, temperature=1.5)\n",
    "    \n",
    "    generated_utterances =  [tokenizer.decode(o[len(input_ids[0]):]) for o in output]\n",
    "    return generated_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79442276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it's quite hot out in the sun. i'm gonna have to use the restroom. m : what's your name?..\n",
      "--------------------------------------------------\n",
      " i want to get off at the airport but i can't because i haven't got a ticket. m : i know i should get you a ticket\n",
      "--------------------------------------------------\n",
      " i'm not wearing any sunglasses. i'm sorry. m : i know you have no idea what you're talking about. but why don '\n",
      "--------------------------------------------------\n",
      "[\" it's quite hot out in the sun. i'm gonna have to use the restroom. m : what's your name?..\", \" i want to get off at the airport but i can't because i haven't got a ticket. m : i know i should get you a ticket\", \" i'm not wearing any sunglasses. i'm sorry. m : i know you have no idea what you're talking about. but why don '\"]\n"
     ]
    }
   ],
   "source": [
    "def augment_samples(sample):\n",
    "    #for sample in samples:\n",
    "        utterances = generate_utterances_for_sample(sample)\n",
    "        for ut in utterances:\n",
    "            print(ut)\n",
    "            print(\"-\" * 50)\n",
    "        sample.generated_utterances = utterances\n",
    "        print(sample.generated_utterances)\n",
    "\n",
    "augment_samples(train_samples[2])\n",
    "\n",
    "#for sample in samples[:1]:\n",
    "    #print(sample)\n",
    "    #print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dfcb061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f : why have you got your sunglasses on ? it 's not that sunny outside . m : i know it 's quite cool today . but i 've got a terrible headache and my eyes hurt in the light .\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[2].article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfb33811",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m   time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mprompt_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [31], line 9\u001b[0m, in \u001b[0;36mprompt_llm\u001b[0;34m(prompt, max_tokens, temperature, stop)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_llm\u001b[39m(prompt, max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 9\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl2021/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl2021/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl2021/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl2021/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl2021/lib/python3.10/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your key\"\n",
    "gpt_version = \"gpt-3.5-turbo\"\n",
    "text = train_samples[2].article\n",
    "\n",
    "def prompt_llm(prompt, max_tokens = 64, temperature=0, stop=None):\n",
    "  response = openai.Completion.create(engine=gpt_version, prompt=prompt,   max_tokens=max_tokens, temperature=temperature, stop=stop)\n",
    "  time.sleep(1)\n",
    "  return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "prompt_llm(text, max_tokens = 30, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7320b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dl2021')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f577ceda18e184c522bf1777ca790ef8361f6159bfa2fedc6f5bcc32669a144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
